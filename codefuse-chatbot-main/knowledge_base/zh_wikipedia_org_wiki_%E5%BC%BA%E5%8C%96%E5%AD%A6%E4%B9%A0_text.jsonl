{"url": "https://zh.wikipedia.org/wiki/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0", "host_url": "https://zh.wikipedia.org", "title": "强化学习 - 维基百科，自由的百科全书", "all_text": "\n\n强化学习 - 维基百科，自由的百科全书\n\n跳转到内容\n\n主菜单\n\n主菜单\n移至侧栏\n隐藏\n\n导航\n\n\n首页分类索引特色内容新闻动态最近更改随机条目\n\n帮助\n\n\n帮助维基社群方针与指引互助客栈知识问答字词转换IRC即时聊天联络我们关于维基百科\n\n搜索\n\n搜索\n\n资助维基百科\n\n外观\n\n创建账号\n\n登录\n\n个人工具\n\n创建账号 登录\n\n未登录编辑者的页面 了解详情\n\n贡献讨论\n\n目录\n移至侧栏\n隐藏\n\n序言\n\n1\n介绍\n\n2\n常用算法\n\n3\n探索机制\n\n4\n参考文献\n\n开关目录\n\n强化学习\n\n38种语言\n\nالعربيةБългарскиবাংলাBosanskiCatalàکوردیČeštinaDeutschΕλληνικάEnglishEspañolEestiEuskaraفارسیSuomiFrançaisעבריתՀայերենBahasa IndonesiaItaliano日本語한국어Bahasa MelayuNederlandsNorsk bokmålଓଡ଼ିଆPolskiRuna SimiРусскийSimple EnglishSlovenščinaСрпски / srpskiSvenskaTürkçeУкраїнськаTiếng Việt吴语粵語\n\n编辑链接\n\n条目讨论\n\n不转换\n\n不转换简体繁體大陆简体香港繁體澳門繁體大马简体新加坡简体臺灣正體\n\n阅读编辑查看历史\n\n工具\n\n工具\n移至侧栏\n隐藏\n\n操作\n\n\n阅读编辑查看历史\n\n常规\n\n\n链入页面相关更改上传文件特殊页面固定链接页面信息引用此页获取短链接下载二维码维基数据项目\n\n打印/导出\n\n\n下载为PDF打印页面\n\n在其他项目中\n\n\n维基共享资源\n\n外观\n移至侧栏\n隐藏\n\n维基百科，自由的百科全书\n\n机器学习与数据挖掘\n范式\n监督学习\n無監督學習\n線上機器學習\n元学习（英语：Meta-learning (computer science)）\n半监督学习\n自监督学习\n强化学习\n基于规则的机器学习（英语：Rule-based machine learning）\n量子機器學習\n\n问题\n统计分类\n生成模型\n迴歸分析\n聚类分析\n降维\n密度估计（英语：density estimation）\n异常检测\n数据清洗\n自动机器学习\n关联规则学习\n語意分析\n结构预测（英语：Structured prediction）\n特征工程\n表征学习\n排序学习（英语：Learning to rank）\n语法归纳（英语：Grammar induction）\n本体学习（英语：Ontology learning）\n多模态学习（英语：Multimodal learning）\n\n监督学习(分类 · 回归)\n学徒学习（英语：Apprenticeship learning）\n决策树学习\n集成学习\nBagging\n提升方法\n随机森林\nk-NN\n線性回歸\n朴素贝叶斯\n人工神经网络\n邏輯斯諦迴歸\n感知器\n相关向量机（RVM）\n支持向量机（SVM）\n迁移学习\n微调\n\n聚类分析\nBIRCH\nCURE算法（英语：CURE algorithm）\n层次\nk-平均\nFuzzy\n期望最大化（EM）\nDBSCAN\nOPTICS\n均值飘移（英语：Mean shift）\n\n降维\n因素分析\nCCA\nICA\nLDA\nNMF（英语：Non-negative matrix factorization）\nPCA\nPGD（英语：Proper generalized decomposition）\nt-SNE（英语：t-distributed stochastic neighbor embedding）\nSDL\n\n结构预测（英语：Structured prediction）\n圖模式\n貝氏網路\n條件隨機域\n隐马尔可夫模型\n\n异常检测\nRANSAC\nk-NN\n局部异常因子（英语：Local outlier factor）\n孤立森林（英语：Isolation forest）\n\n人工神经网络\n自编码器\n認知計算\n深度学习\nDeepDream（英语：DeepDream）\n多层感知器\nRNN\nLSTM\nGRU（英语：Gated recurrent unit）\nESN（英语：Echo state network）\n储备池计算（英语：reservoir computing）\n受限玻尔兹曼机\nGAN\nSOM\nCNN\nU-Net\nTransformer\nVision transformer（英语：Vision transformer）\n脉冲神经网络（英语：Spiking neural network）\nMemtransistor（英语：Memtransistor）\n电化学RAM（英语：Electrochemical RAM）（ECRAM）\n\n强化学习\nQ学习\nSARSA\n时序差分（TD）\n多智能体（英语：Multi-agent reinforcement learning）\nSelf-play（英语：Self-play (reinforcement learning technique)）\nRLHF\n\n与人类学习\n主动学习（英语：Active learning (machine learning)）\n众包\nHuman-in-the-loop（英语：Human-in-the-loop）\n\n模型诊断\n学习曲线（英语：Learning curve (machine learning)）\n\n数学基础\n内核机器（英语：Kernel machines）\n偏差–方差困境（英语：Bias–variance tradeoff）\n计算学习理论（英语：Computational learning theory）\n经验风险最小化\n奥卡姆学习（英语：Occam learning）\nPAC学习（英语：Probably approximately correct learning）\n统计学习\nVC理论\n\n大会与出版物\nNeurIPS\nICML（英语：International Conference on Machine Learning）\nICLR\nML（英语：Machine Learning (journal)）\nJMLR（英语：Journal of Machine Learning Research）\n\n相关条目\n人工智能术语（英语：Glossary of artificial intelligence）\n机器学习研究数据集列表（英语：List of datasets for machine-learning research）\n机器学习概要（英语：Outline of machine learning）\n查论编\n强化学习（英語：Reinforcement learning，簡稱RL）是机器学习中的一个领域，强调如何基于环境而行动，以取得最大化的预期利益[1]。强化学习是除了监督学习和非监督学习之外的第三种基本的机器学习方法。与监督学习不同的是，强化学习不需要带标签的输入输出对，同时也无需对非最优解的精确地纠正。其关注点在于寻找探索（对未知领域的）和利用（对已有知识的）的平衡[2]，强化学习中的“探索-利用”的交换，在多臂老虎机（英语：multi-armed bandit）问题和有限MDP中研究得最多。\n其灵感来源于心理学中的行为主义理论，即有机体如何在环境给予的奖励或惩罚的刺激下，逐步形成对刺激的预期，产生能获得最大利益的习惯性行为。这个方法具有普适性，因此在其他许多领域都有研究，例如博弈论、控制论、运筹学、信息论、仿真优化、多智能体系统、群体智能、统计学以及遗传算法。在运筹学和控制理论研究的语境下，强化学习被称作“近似动态规划”（approximate dynamic programming，ADP）。在最优控制理论中也有研究这个问题，虽然大部分的研究是关于最优解的存在和特性，并非是学习或者近似方面。在经济学和博弈论中，强化学习被用来解释在有限理性的条件下如何出现平衡。\n在机器学习问题中，环境通常被抽象为马尔可夫决策过程（Markov decision processes，MDP），因为很多强化学习算法在这种假设下才能使用动态规划的方法[3]。传统的动态规划方法和强化学习算法的主要区别是，后者不需要关于MDP的知识，而且针对无法找到确切方法的大规模MDP。[4]\n\n介绍[编辑]\n强化学习的典型框架：智能体在环境中采取一种行为，环境将其转换为一次回报和一种状态表示，随后反馈给智能体。\n由于其通用性很强，强化学习已经在诸如博弈论、控制论、运筹学、信息论、仿真优化、多智能体、群体智能和统计学等领域有了深入研究。在运筹学和控制文献中，强化学习被称为近似动态规划或神经动态规划。强化学习所感兴趣的问题在最优控制（一种关注最优解的存在性、表示和求解的理论，但较少涉及学习和近似）中也有所研究，尤其是环境的数学模型难以求得的时候。在经济学和博弈论中，强化学习可能被用来解释在有限的理性（rationality）下如何达到平衡状态。\n基本的强化学习被建模为马尔可夫决策过程：\n\n环境状态的集合\n\nS\n\n{\\displaystyle S}\n\n;\n动作的集合\n\nA\n\n{\\displaystyle A}\n\n;\n在状态之间转换的规则（转移概率矩阵）\n\nP\n\n{\\displaystyle P}\n\n；\n规定转换后“即时奖励”的规则（奖励函数）\n\nR\n\n{\\displaystyle R}\n\n；\n描述主体能够观察到什么的规则。\n规则通常是随机的。主体通常可以观察即时奖励和最后一次转换。在许多模型中，主体被假设为可以观察现有的环境状态，这种情况称为“完全可观测”（full observability），反之则称为“部分可观测”（partial observability）。通常，主体被允许的动作是有限的，例如，在棋盤中棋子只能上、下、左、右移動，或是使用的钱不能多于所拥有的。\n强化学习的主体与环境基于离散的时间步作用。在每一个时间\n\nt\n\n{\\displaystyle t}\n\n，主体接收到一个观测\n\no\n\nt\n\n{\\displaystyle o_{t}}\n\n，通常其中包含奖励\n\nr\n\nt\n\n{\\displaystyle r_{t}}\n\n。然后，它从允许的集合中选择一个动作\n\na\n\nt\n\n{\\displaystyle a_{t}}\n\n，然后送出到环境中去。环境则变化到一个新的状态\n\ns\n\nt\n+\n1\n\n{\\displaystyle s_{t+1}}\n\n，然后决定了和这个变化\n\n(\n\ns\n\nt\n\n,\n\na\n\nt\n\n,\n\ns\n\nt\n+\n1\n\n)\n\n{\\displaystyle (s_{t},a_{t},s_{t+1})}\n\n相关联的奖励\n\nr\n\nt\n+\n1\n\n{\\displaystyle r_{t+1}}\n\n。强化学习主体的目标，是得到尽可能多的奖励。主体选择的动作是其历史的函数，它也可以选择随机的动作。\n将这个主体的表现和自始自终以最优方式行动的主体相比较，它们之间的行动差异产生了“悔过”的概念。如果要接近最优的方案来行动，主体必须根据它的长时间行动序列进行推理：例如，要最大化我的未来收入，我最好现在去上学，虽然这样行动的即时货币奖励为负值。\n因此，强化学习对于包含长期反馈的问题比短期反馈的表现更好。它在许多问题上得到应用，包括机器人控制、电梯调度、电信通讯、双陆棋和西洋跳棋。[5]\n强化学习的强大能力来源于两个方面：使用样本来优化行为，使用函数近似来描述复杂的环境。它们使得强化学习可以使用在以下的复杂环境中：\n\n模型的环境已知，且解析解不存在；\n仅仅给出环境的模拟模型（模拟优化方法的问题）[6]\n从环境中获取信息的唯一办法是和它互动。前两个问题可以被考虑为规划问题，而最后一个问题可以被认为是genuine learning问题。使用强化学习的方法，这两种规划问题都可以被转化为机器学习问题。\n常用算法[编辑]\n蒙特卡洛学习 Monte-Carlo Learning\nTemporal-Difference Learning\nSARSA算法\nQ学习\n探索机制[编辑]\n强化学习需要比较聪明的探索机制，直接随机的对动作进行采样的方法性能比较差。虽然小规模的马氏过程已经被认识的比较清楚，这些性质很难在状态空间规模比较大的时候适用，这个时候相对简单的探索机制是更加现实的。\n其中的一种方法就是\n\nϵ\n\n{\\displaystyle \\epsilon }\n\n-貪婪演算法，这种方法会以比较大的概率(1-\n\nϵ\n\n{\\displaystyle \\epsilon }\n\n)去选择现在最好的动作。如果没有选择最优动作，就在剩下的动作中随机选择一个。\n\nϵ\n\n{\\displaystyle \\epsilon }\n\n在这里是一个可调节的参数，更小的\n\nϵ\n\n{\\displaystyle \\epsilon }\n\n意味着算法会更加贪心。[7]\n\n参考文献[编辑]\n\nScholia上有關强化学习的信息\n\n^ Hu, J.; Niu, H.; Carrasco, J.; Lennox, B.; Arvin, F. Voronoi-Based Multi-Robot Autonomous Exploration in Unknown Environments via Deep Reinforcement Learning. IEEE Transactions on Vehicular Technology. 2020, 69 (12): 14413-14423  [2021-03-21]. （原始内容存档于2021-08-13）.\n\n^ Kaelbling, Leslie P.; Littman, Michael L.; Moore, Andrew W. Reinforcement Learning: A Survey. Journal of Artificial Intelligence Research. 1996, 4: 237–285. S2CID 1708582. arXiv:cs/9605103 . doi:10.1613/jair.301. （原始内容存档于2001-11-20）.\n\n^ van Otterlo, M.; Wiering, M. Reinforcement Learning and Markov Decision Processes. Reinforcement Learning. Adaptation, Learning, and Optimization 12. 2012: 3–42. ISBN 978-3-642-27644-6. doi:10.1007/978-3-642-27645-3_1.\n\n^ 强化学习：原理与Python实现. 北京. 2019: 16–19. ISBN 9787111631774.\n\n^ Sutton1998|Sutton and Barto 1998 Chapter 11\n\n^ Gosavi, Abhijit. Simulation-based Optimization: Parametric Optimization Techniques and Reinforcement. Springer. 2003  [2015-08-19]. ISBN 1-4020-7454-9. （原始内容存档于2012-06-15）.\n\n^ Tokic, Michel; Palm, Günther, Value-Difference Based Exploration: Adaptive Control Between Epsilon-Greedy and Softmax, KI 2011: Advances in Artificial Intelligence (PDF), Lecture Notes in Computer Science 7006, Springer: 335–346, 2011  [2018-09-03], ISBN 978-3-642-24455-1, （原始内容存档 (PDF)于2018-11-23）\n\n查论编机器学习同数据挖掘主題基本概念学习 · 图灵测试 · 運算學習論數學模型迴歸模型 · 人工神经网络（深度学习） · 生成对抗网络 · Transformer模型 · 大语言模型 · 決策樹 · 貝氏網路 · 支持向量机 · 关联规则学习學習範式机器学习 ·  深度学习 · 迁移学习 ·  微调 (深度学习) ·  监督学习 ·  半监督学习 ·  無監督學習 · 强化学习 · Q学习 · 遺傳演算法主要應用统计分类 · 表征学习 · 降维 · 聚类分析 · 异常检测相關領域计算科學 · 人工智能 · 通用人工智慧 · 生成式人工智慧 · 提示工程 · 统计学 · 數據科學 · 计算机科学 · 信息与计算科学 · 神经科学 · 认知科学\n查论编生成式人工智能概念\n自动编码器\n变分自编码器\n聊天机器人\n列表\n生成对抗网络\nGPT\n幻觉\n模型崩溃（英语：Model collapse）\n大型语言模型\n基础模型\n微调\n检索增强生成（英语：Retrieval-augmented generation）\nBERT\n对话程式\n机器学习\n深度学习\n强化学习\nRLHF\n监督学习\n自监督学习\n神经网络\n提示工程\nTransformer模型\nvision transformer（英语：Vision transformer）\n向量数据库（英语：Vector database）\n词嵌入\n模型文本\nClaude\nGemini\nGPT-3\nGPT-4\no1（英语：OpenAI o1）\nLLaMA\n图像\nDALL-E\nMidjourney\nStable Diffusion\n视频（維基數據所列：文本到视频生成模型）\nSora\n音乐\nSuno AI\nUdio（英语：Udio）\n多模态（英语：Multimodal learning）\nGPT-4o\n企业与产品\nAnthropic\nApple Intelligence\nGitHub Copilot\nGoogle DeepMind\nHugging Face\nMeta AI（英语：Meta AI）\nMicrosoft Copilot\nMistral AI\nOpenAI\nChatGPT\nPerplexity\nCategory\n\n检索自“https://zh.wikipedia.org/w/index.php?title=强化学习&oldid=84364156”\n分类：​机器学习人工智能機器學習演算法强化学习隐藏分类：​含有英語的條目\n\n本页面最后修订于2024年9月26日 (星期四) 17:44。\n本站的全部文字在知识共享 署名-相同方式共享 4.0协议之条款下提供，附加条款亦可能应用。（请参阅使用条款）\nWikipedia®和维基百科标志是维基媒体基金会的注册商标；维基™是维基媒体基金会的商标。\n维基媒体基金会是按美国国內稅收法501(c)(3)登记的非营利慈善机构。\n\n隐私政策\n关于维基百科\n免责声明\n行为准则\n开发者\n统计\nCookie声明\n手机版视图\n\n"}
